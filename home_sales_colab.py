# -*- coding: utf-8 -*-
"""Home_Sales_starter_code_colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15zddVY5Qc5YRL1Ao4Lv7EL9rWNTn-Pg1
"""

import os
# Set the Spark version
spark_version = 'spark-3.4.0'
os.environ['SPARK_VERSION'] = spark_version

# Install Spark and Java
!apt-get update -y
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz
!tar xf $SPARK_VERSION-bin-hadoop3.tgz
!pip install -q findspark

# Set Environment Variables
os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'
os.environ['SPARK_HOME'] = f'/content/{spark_version}-bin-hadoop3'

# Start a SparkSession
import findspark
findspark.init()

from pyspark.sql import SparkSession

spark = SparkSession.builder.master("local[*]").appName("Colab Spark").getOrCreate()
spark

# Import packages
from pyspark.sql import SparkSession
import time

# Create a SparkSession
spark = SparkSession.builder.appName("SparkSQL").getOrCreate()

# 1. Read in the AWS S3 bucket into a DataFrame.
from pyspark import SparkFiles
url = "https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv"
spark.sparkContext.addFile(url)
df = spark.read.csv("file://" + SparkFiles.get("home_sales_revised.csv"), header=True, inferSchema=True)

# 2. Create a temporary view of the DataFrame.
df.createOrReplaceTempView("home_sales")

result = spark.sql("SELECT * FROM home_sales LIMIT 10")
result.show()

# 3. What is the average price for a four bedroom house sold per year, rounded to two decimal places?
query = """
    SELECT
        YEAR(date) AS year,
        ROUND(AVG(price), 2) AS average_price
    FROM home_sales
    WHERE bedrooms = 4
    GROUP BY year
    ORDER BY year
"""
result = spark.sql(query)
result.show()

# 4. What is the average price of a home for each year the home was built,
# that have 3 bedrooms and 3 bathrooms, rounded to two decimal places?
query = """
    SELECT
        date_built AS year_built,
        ROUND(AVG(price), 2) AS average_price
    FROM home_sales
    WHERE bedrooms = 3 AND bathrooms = 3
    GROUP BY date_built
    ORDER BY date_built
"""
result = spark.sql(query)
result.show()

# 5. What is the average price of a home for each year the home was built,
# that have 3 bedrooms, 3 bathrooms, with two floors,
# and are greater than or equal to 2,000 square feet, rounded to two decimal places?
query = """
    SELECT
        date_built AS year_built,
        ROUND(AVG(price), 2) AS average_price
    FROM home_sales
    WHERE bedrooms = 3 AND bathrooms = 3 AND floors = 2 AND sqft_living >= 2000
    GROUP BY date_built
    ORDER BY date_built
"""
result = spark.sql(query)
result.show()

# 6. What is the average price of a home per "view" rating, rounded to two decimal places,
# having an average home price greater than or equal to $350,000? Order by descending view rating.
# Although this is a small dataset, determine the run time for this query.

start_time = time.time()

query = """
    SELECT
        view,
        ROUND(AVG(price), 2) AS average_price
    FROM home_sales
    GROUP BY view
    HAVING ROUND(AVG(price), 2) >= 350000
    ORDER BY view DESC
"""
result = spark.sql(query)
result.show()

print("--- %s seconds ---" % (time.time() - start_time))

# 7. Cache the the temporary table home_sales.
spark.catalog.cacheTable("home_sales")

# 8. Check if the table is cached.
spark.catalog.isCached('home_sales')

# 9. Using the cached data, run the last query above, that calculates
# the average price of a home per "view" rating, rounded to two decimal places,
# having an average home price greater than or equal to $350,000.
# Determine the runtime and compare it to the uncached runtime.

start_time = time.time()

query = """
    SELECT
        view,
        ROUND(AVG(price), 2) AS average_price
    FROM home_sales
    GROUP BY view
    HAVING ROUND(AVG(price), 2) >= 350000
    ORDER BY view DESC
"""
result = spark.sql(query)
result.show()

print("--- %s seconds ---" % (time.time() - start_time))

# 10. Partition by the "date_built" field on the formatted parquet home sales data
output_path = "/content/home_sales_partitioned"
df.write.partitionBy("date_built").parquet(output_path)

print(f"Data partitioned by 'date_built' and saved to {output_path}")

# 11. Read the parquet formatted data.
input_path = "/content/home_sales_partitioned"
parquet_df = spark.read.parquet(input_path)

# Show the first few rows of the DataFrame
parquet_df.show()

# 12. Create a temporary table for the parquet data.
parquet_df.createOrReplaceTempView("home_sales_parquet")

# Verify that the temporary table has been created
tables = spark.catalog.listTables()
print("Temporary tables in the catalog:")
for table in tables:
    print(f"Table name: {table.name}, Table type: {table.tableType}")

# Optionally, show a few rows from the temporary table
spark.sql("SELECT * FROM home_sales_parquet LIMIT 5").show()

# 13. Using the parquet DataFrame, run the last query above, that calculates
# the average price of a home per "view" rating, rounded to two decimal places,
# having an average home price greater than or equal to $350,000.
# Determine the runtime and compare it to the cached runtime.

start_time = time.time()

query = """
    SELECT
        view,
        ROUND(AVG(price), 2) AS average_price
    FROM home_sales_parquet
    GROUP BY view
    HAVING ROUND(AVG(price), 2) >= 350000
    ORDER BY view DESC
"""
result = spark.sql(query)
result.show()

print("--- %s seconds ---" % (time.time() - start_time))

# 14. Uncache the home_sales temporary table.
spark.catalog.uncacheTable("home_sales")

# 15. Check if the home_sales is no longer cached
is_cached = spark.catalog.isCached("home_sales")
print(f"Is 'home_sales' table still cached? {is_cached}")

